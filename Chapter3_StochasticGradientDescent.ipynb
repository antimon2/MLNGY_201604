{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 第3章 確率的勾配降下法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 勾配降下法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "勾配："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\nabla E \\equiv \\frac{\\partial E}{\\partial {\\bf w}} = \\left[ \\frac{\\partial E}{\\partial w_1} \\frac{\\partial E}{\\partial w_2} \\dots \\frac{\\partial E}{\\partial w_M} \\right]^T\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "勾配降下法：\n",
    "\n",
    "${\\bf w}$ を「負の勾配方向 $-\\nabla E$ に少し動かす」ことを繰り返して、その極小点を求める方法。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ {\\bf w}^{(t+1)} = {\\bf w}^{(t)} - \\epsilon \\nabla E $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "この $\\epsilon$ を **学習係数**（**learning rate**）と呼ぶ。\n",
    "\n",
    "+ $\\epsilon$ が小さい ⇒ 学習にかかる時間が大きくなる\n",
    "+ $\\epsilon$ が大きい ⇒ 極小点に収束しないことがある"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "※ TensorFlow では `tf.train.GradientDescentOptimizer` というクラスが用意されている。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "optimizer = tf.train.GradientDescentOptimizer(0.01)\n",
    "train_step = optimizer.minimize(cross_entropy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 確率的勾配降下法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**バッチ学習**（**batch learning**）（もしくは **エポック学習**（**epoch learning**））：  \n",
    "3.1 節で見た（全訓練データを利用した）勾配降下法による学習。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**確率的勾配降下法**（**stochastic gradient descent**）（もしくは **逐次的勾配降下法**（**sequential gradient descent**））：  \n",
    "訓練データいくつか（極端には1つ）ずつ繰り返し適用していく勾配降下法。  \n",
    "**SGD** と略される。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "E({\\bf w}) = \\sum_n E_n({\\bf w}) \\\\\n",
    "{\\bf w}^{(t+1)} = {\\bf w}^{(t)} - \\epsilon \\nabla E_n \n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| | バッチ学習 | SGD |\n",
    "| --- | --- | --- |\n",
    "| **冗長性への対応** | データ量に比例して計算コストがかかる | 学習内容・計算量に影響なし |\n",
    "| **局所解** | 望まない局所的な極小解に陥るリスクが高い | 局所的な極小解に陥るリスクが低い |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "その他 SGD の利点：\n",
    "\n",
    "+ 学習の途中経過を随時確認出来る。\n",
    "+ オンライン学習（＝データの収集と最適化の計算を同時並行に行える）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "※ TensorFlow では、`tf.train.GradientDescentOptimizer` から `train_step` を定義したら、それをループで訓練データ1つずつ適用していけば SGD が実現出来る。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 ミニバッチ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ミニバッチ**（**minibatch**）:  \n",
    "SGD を「訓練データ1つずつ」ではなく「いくつか（10〜100）ずつ」適用する場合の、そのひとまとめにしたサンプル集合。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "※ TensorFlow では、`tf.train.GradientDescentOptimizer` から `train_step` を定義したら、バッチサイズを決めてループで訓練データをその個数ずつ取り出して適用していけば ミニバッチによるSGD が実現出来る。  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "コード例："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in range(1000):\n",
    "    batch_xs, batch_ys = training_data.next_batch(100)\n",
    "    sess.run(train_step, feed_dict={x: batch_xs, y_: batch_ys})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 汎化性能と過適合"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 過適合の緩和"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.5.1 正則化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.5.2 重みの制約"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.5.3 ドロップアウト"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.6 学習のトリック"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### 3.6.1 データの正則化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.6.2 データ拡張"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.6.3 複数ネットの平均"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.6.4 学習係数の決め方"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.6.5 モメンタム"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.6.6 重みの初期化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.6.7 サンプルの順序"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TensorFlow (Python 2)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
