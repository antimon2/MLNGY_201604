{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 第3章 確率的勾配降下法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 勾配降下法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "勾配："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\nabla E \\equiv \\frac{\\partial E}{\\partial {\\bf w}} = \\left[ \\frac{\\partial E}{\\partial w_1} \\frac{\\partial E}{\\partial w_2} \\dots \\frac{\\partial E}{\\partial w_M} \\right]^T\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "勾配降下法：\n",
    "\n",
    "${\\bf w}$ を「負の勾配方向 $-\\nabla E$ に少し動かす」ことを繰り返して、その極小点を求める方法。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ {\\bf w}^{(t+1)} = {\\bf w}^{(t)} - \\epsilon \\nabla E $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "この $\\epsilon$ を **学習係数**（**learning rate**）と呼ぶ。\n",
    "\n",
    "+ $\\epsilon$ が小さい ⇒ 学習にかかる時間が大きくなる\n",
    "+ $\\epsilon$ が大きい ⇒ 極小点に収束しないことがある"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "※ TensorFlow では `tf.train.GradientDescentOptimizer` というクラスが用意されている。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "optimizer = tf.train.GradientDescentOptimizer(0.01) # 引数は学習係数（learning rate）\n",
    "train_step = optimizer.minimize(cross_entropy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 確率的勾配降下法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**バッチ学習**（**batch learning**）（もしくは **エポック学習**（**epoch learning**））：  \n",
    "3.1 節で見た（全訓練データを利用した）勾配降下法による学習。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**確率的勾配降下法**（**stochastic gradient descent**）（もしくは **逐次的勾配降下法**（**sequential gradient descent**））：  \n",
    "訓練データいくつか（極端には1つ）ずつ繰り返し適用していく勾配降下法。  \n",
    "**SGD** と略される。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "E({\\bf w}) = \\sum_n E_n({\\bf w}) \\\\\n",
    "{\\bf w}^{(t+1)} = {\\bf w}^{(t)} - \\epsilon \\nabla E_n \n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| | バッチ学習 | SGD |\n",
    "| --- | --- | --- |\n",
    "| **冗長性への対応** | データ量に比例して計算コストがかかる | 学習内容・計算量に影響なし |\n",
    "| **局所解** | 望まない局所的な極小解に陥るリスクが高い | 局所的な極小解に陥るリスクが低い |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "その他 SGD の利点：\n",
    "\n",
    "+ 学習の途中経過を随時確認出来る。\n",
    "+ オンライン学習（＝データの収集と最適化の計算を同時並行に行える）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "※ TensorFlow では、`tf.train.GradientDescentOptimizer` から `train_step` を定義したら、それをループで訓練データ1つずつ適用していけば SGD が実現出来る。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 ミニバッチ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ミニバッチ**（**minibatch**）:  \n",
    "SGD を「訓練データ1つずつ」ではなく「いくつか（10〜100）ずつ」適用する場合の、そのひとまとめにしたサンプル集合。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "※ TensorFlow では、`tf.train.GradientDescentOptimizer` から `train_step` を定義したら、バッチサイズを決めてループで訓練データをその個数ずつ取り出して適用していけば ミニバッチによるSGD が実現出来る。  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "コード例："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in range(1000):\n",
    "    batch_xs, batch_ys = training_data.next_batch(100)\n",
    "    sess.run(train_step, feed_dict={x: batch_xs, y_: batch_ys})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 汎化性能と過適合"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**訓練誤差**（**training error**）：  \n",
    "訓練データに対する誤差（＝学習結果（もしくは途中経過）の誤差関数の値）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**汎化誤差**（**generalization error**）：  \n",
    "サンプルの母集団に対する誤差（≒未知のデータに対してどれくらい正しい推定が行えているか）の期待値"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**テスト誤差**（**test error**）：  \n",
    "訓練データとは別に用意した**テストデータ**に対する誤差（＝学習結果のモデルによる誤差関数の値）  \n",
    "汎化誤差の目安として利用"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**過適合**（**overfitting**）（もしくは**過学習**（**overlearning**））：  \n",
    "訓練データに過剰に適合するように学習されて、未知のデータ（もしくはテストデータ）にうまく適合しなくなっている状態。  \n",
    "**学習曲線**を描いたとき、テスト誤差が訓練誤差と乖離してしまっている状態。  \n",
    "テスト誤差が乖離し始めたら学習を打ち切ることを**早期終了**（**early stopping**）（あるいは**早期打ち切り**）と呼ぶ。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 過適合の緩和"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.5.1 正則化"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**正則化**（**regularization**）：  \n",
    "過適合を緩和するための（いくつかの）手法\n",
    "\n",
    "+ 重み減衰\n",
    "+ 重み上限\n",
    "+ ドロップアウト"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.5.2 重みの制約"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**重み減衰**（**weight decay**）：  \n",
    "誤差関数に重みの二乗和（二乗ノルム）を加算する正則化手法。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "E_t({\\bf w}) \\equiv \\frac{1}{N_t} \\sum_{n \\in D_t} E_n({\\bf w}) + \\frac{\\lambda}{2}\\|{\\bf w}\\|^2\n",
    "\\\\\n",
    "{\\bf w}^{(t+1)} = {\\bf w}^{(t)} - \\epsilon \\left(\\frac{1}{N_t} \\nabla E_n + \\lambda {\\bf w}^{(t)} \\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "（$\\lambda$：0.01〜0.00001 くらいの範囲の定数（正則化パラメータと呼ぶこともある））"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**重み上限**：  \n",
    "重みの大きさの上限を制約する正則化手法（詳細略）。  \n",
    "※重み減衰よりを上回る効果がある（らしい）。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.5.3 ドロップアウト"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ドロップアウト**（**dropout**）：  \n",
    "多層ネットワークのユニットを確率的に選別して学習する方法（詳細略）。  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.6 学習のトリック"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### 3.6.1 データの正規化"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**正規化**（**normalization**）（もしくは**標準化**（**standardization**））：  \n",
    "データの平均を0に、分散を1になるよう変換すること。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.6.2 データ拡張"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.6.3 複数ネットの平均"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.6.4 学習係数の決め方"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.6.5 モメンタム"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.6.6 重みの初期化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.6.7 サンプルの順序"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TensorFlow (Python 2)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
