{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 「深層学習」読書会　〜第3章〜"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"text-align:center;font-size:150%;line-height:150%\">2016/04/16 機械学習 名古屋 第3回勉強会</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 第3章 確率的勾配降下法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 3.1 勾配降下法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "勾配："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\nabla E \\equiv \\frac{\\partial E}{\\partial {\\bf w}} = \\left[ \\frac{\\partial E}{\\partial w_1}\\ \\ \\frac{\\partial E}{\\partial w_2}\\ \\  \\dots \\ \\ \\frac{\\partial E}{\\partial w_M} \\right]^T\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "勾配降下法：\n",
    "\n",
    "${\\bf w}$ を「負の勾配方向 $-\\nabla E$ に少し動かす」ことを繰り返して、その極小点を求める方法。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ {\\bf w}^{(t+1)} = {\\bf w}^{(t)} - \\epsilon \\nabla E $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "この $\\epsilon$ を **学習係数**（**learning rate**）と呼ぶ。\n",
    "\n",
    "+ $\\epsilon$ が小さい ⇒ 学習にかかる時間が大きくなる\n",
    "+ $\\epsilon$ が大きい ⇒ 極小点に収束しないことがある"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "※ TensorFlow では `tf.train.GradientDescentOptimizer` というクラスが用意されている。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "optimizer = tf.train.GradientDescentOptimizer(0.01)\n",
    "# 引数は学習係数（learning rate）\n",
    "train_step = optimizer.minimize(cross_entropy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 3.2 確率的勾配降下法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**バッチ学習**（**batch learning**）（もしくは **エポック学習**（**epoch learning**））：  \n",
    "3.1 節で見た（全訓練データを利用した）勾配降下法による学習。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**確率的勾配降下法**（**stochastic gradient descent**）（もしくは **逐次的勾配降下法**（**sequential gradient descent**））：  \n",
    "訓練データいくつか（極端には1つ）ずつ繰り返し適用していく勾配降下法。  \n",
    "**SGD** と略される。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "E({\\bf w}) = \\sum_n E_n({\\bf w}) \\\\\n",
    "{\\bf w}^{(t+1)} = {\\bf w}^{(t)} - \\epsilon \\nabla E_n \n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "| | バッチ学習 | SGD |\n",
    "| --- | --- | --- |\n",
    "| **冗長性への対応** | データ量に比例して計算コストがかかる | 学習内容・計算量に影響なし |\n",
    "| **局所解** | 望まない局所的な極小解に陥るリスクが高い | 局所的な極小解に陥るリスクが低い |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "その他 SGD の利点：\n",
    "\n",
    "+ 学習の途中経過を随時確認出来る。\n",
    "+ オンライン学習（＝データの収集と最適化の計算を同時並行に行える）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "※ TensorFlow では、`tf.train.GradientDescentOptimizer` から `train_step` を定義したら、それをループで訓練データ1つずつ適用していけば SGD が実現出来る。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 3.3 ミニバッチ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**ミニバッチ**（**minibatch**）:  \n",
    "SGD を「訓練データ1つずつ」ではなく「いくつか（10〜100）ずつ」適用する場合の、そのひとまとめにしたサンプル集合。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "※ TensorFlow では、`tf.train.GradientDescentOptimizer` から `train_step` を定義したら、バッチサイズを決めてループで訓練データをその個数ずつ取り出して適用していけば ミニバッチによるSGD が実現出来る。  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "コード例："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in range(1000):\n",
    "    batch_xs, batch_ys = training_data.next_batch(100)\n",
    "    sess.run(train_step, feed_dict={x: batch_xs, y_: batch_ys})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 3.4 汎化性能と過適合"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**訓練誤差**（**training error**）：  \n",
    "訓練データに対する誤差（＝学習結果（もしくは途中経過）の誤差関数の値）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**汎化誤差**（**generalization error**）：  \n",
    "サンプルの母集団に対する誤差（≒未知のデータに対してどれくらい正しい推定が行えているか）の期待値"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**テスト誤差**（**test error**）：  \n",
    "訓練データとは別に用意した**テストデータ**に対する誤差（＝学習結果のモデルによる誤差関数の値）  \n",
    "汎化誤差の目安として利用"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**過適合**（**overfitting**）（もしくは**過学習**（**overlearning**））：  \n",
    "訓練データに過剰に適合するように学習されて、未知のデータ（もしくはテストデータ）にうまく適合しなくなっている状態。  \n",
    "**学習曲線**を描いたとき、テスト誤差が訓練誤差と乖離してしまっている状態。  \n",
    "テスト誤差が乖離し始めたら学習を打ち切ることを**早期終了**（**early stopping**）（あるいは**早期打ち切り**）と呼ぶ。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 3.5 過適合の緩和"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### 3.5.1 正則化"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "**正則化**（**regularization**）：  \n",
    "過適合を緩和するための（いくつかの）手法\n",
    "\n",
    "+ 重み減衰\n",
    "+ 重み上限\n",
    "+ ドロップアウト"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### 3.5.2 重みの制約"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**重み減衰**（**weight decay**）：  \n",
    "誤差関数に重みの二乗和（二乗ノルム）を加算する正則化手法。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "E_t({\\bf w}) \\equiv \\frac{1}{N_t} \\sum_{n \\in D_t} E_n({\\bf w}) + \\frac{\\lambda}{2}\\|{\\bf w}\\|^2\n",
    "\\\\\n",
    "{\\bf w}^{(t+1)} = {\\bf w}^{(t)} - \\epsilon \\left(\\frac{1}{N_t} \\nabla E_n + \\lambda {\\bf w}^{(t)} \\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "（$\\lambda$：0.01〜0.00001 くらいの範囲の定数（正則化パラメータと呼ぶこともある））"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**重み上限**：  \n",
    "重みの大きさの上限を制約する正則化手法（詳細略）。  \n",
    "※重み減衰よりを上回る効果がある（らしい）。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### 3.5.3 ドロップアウト"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**ドロップアウト**（**dropout**）：  \n",
    "多層ネットワークのユニットを確率的に選別して学習する方法（詳細略）。  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 3.6 学習のトリック"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### 3.6.1 データの正規化"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**正規化**（**normalization**）（もしくは**標準化**（**standardization**））：  \n",
    "データの平均を0に（および分散を1に）なるよう変換すること。\n",
    "データに偏りがある場合に実行する（詳細略）。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### 3.6.2 データ拡張"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**データ拡張**（**data augmentation**）：  \n",
    "データを水増しすること。  \n",
    "訓練データが少ない場合、かつデータの「ばらつき方」が予想出来る場合に有効（詳細略）。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### 3.6.3 複数ネットの平均"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**モデル平均**（**model averaging**）：  \n",
    "入力層と出力層が同じ複数のニューラルネットで別々に学習して、その平均を取ること。  \n",
    "3.5.3節の「ドロップアウト」もモデル平均と同じ効果が得られる（詳細略）。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### 3.6.4 学習係数の決め方"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ 「手動で（試行錯誤的に）選ぶ」という方法が一般的\n",
    "+ 最初は $\\epsilon$ を大きめにとって、学習が進むにつれて少しずつ小さくする方法：\n",
    "  + イテレーション回数に比例して小さくする（ $\\epsilon = \\epsilon_0 - \\alpha t$ ）\n",
    "  + ある程度学習が進んだら $1/10$ にする（ $\\epsilon = \\epsilon_0 / 10$ ）（これを繰り返す）\n",
    "+ 層ごとに学習係数を変える（ReLU を利用する場合にはあまり有効ではない）\n",
    "+ AdaGrad（TensorFlow にも `tf.train.AdagradOptimizer` が用意されている）（詳細略）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### 3.6.5 モメンタム"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**モメンタム**（**momentum**）：  \n",
    "勾配降下法において、重みの修正量に、前回の重みの修正量のいくらかを加算する方法。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$\n",
    "{\\bf w}^{(t+1)} = {\\bf w}^{(t)} - \\epsilon \\nabla E_n + \\mu \\Delta {\\bf w}^{(t-1)}\n",
    "$$\n",
    "ただし\n",
    "$$\n",
    "\\Delta {\\bf w}^{(t-1)} \\equiv {\\bf w}^{(t-1)} - {\\bf w}^{(t-2)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "TensorFlow には `tf.train.MomentumOptimizer` が用意されている。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "optimizer = tf.train.MomentumOptimizer(0.1, 0.9)\n",
    "# 第1引数は学習係数（learning rate）、第2引数はモメンタムのハイパーパラメータ（先ほどの式の μ の値）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### 3.6.6 重みの初期化"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "重みは、ガウス分布から生成したランダム値（$w_{ij} \\sim N(0, \\sigma^2)$）で初期化する（のが一般的）。  \n",
    "バイアスは $0$ で初期化する（のが一般的）。  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "TensorFlow では、重みおよびバイアスを定義するときに `tf.Variable()` の引数で指定（2.1節参照）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "W = tf.Variable(tf.random_normal([64, 10], mean=0.0, stddev=0.05))\n",
    "b = tf.Variable(tf.zeros([10]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### 3.6.7 サンプルの順序"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "《略》"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "TensorFlow (Python 2)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
